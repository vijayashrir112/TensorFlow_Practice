{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "3\n",
    "os.getcwd() # check your current working diretory\n",
    "4\n",
    "os.chdir(\"C:/Python_DS_Challenges/DataPipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Python_DS_Challenges\\\\DataPipeline'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There are two ways to createa tf.datast.Dataset\n",
    "\n",
    "### 1. Data Source\n",
    "\n",
    "Construct a Dataset from data stored in memory or in files\n",
    "\n",
    "### 2. Data Transformation\n",
    "Construct Datasetfrom one or more data objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective:\n",
    "    \n",
    "-   Load data from a CSV\n",
    "-  Load from numpy and pandas\n",
    "- Save and load data\n",
    "       - tf.example\n",
    "       - TFRecord\n",
    "- Load image data\n",
    "- End Result\n",
    "- Get data in and out of TensorFlow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data from CSV\n",
    "\n",
    "Import libraries, numpy, pandas and tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load two CSVs 1. Train, 2. Evaluation csv.  Both are coming from URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use the Keras Utility from the utils get file as the train.csv and pass in the TRAIN_DATA_URL\n",
    "What this does is download the CSV directly from the web and then it will put it in the place train filepath and test file path\n",
    "This is actually the location of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tf-datasets/titanic/eval.csv\n",
      "16384/13049 [=====================================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\n",
    "TEST_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\"\n",
    "\n",
    "train_file_path = tf.keras.utils.get_file('C:\\\\Python_DS_Challenges\\\\DataPipeline\\\\train.csv', TRAIN_DATA_URL)\n",
    "test_file_path = tf.keras.utils.get_file('C:\\\\Python_DS_Challenges\\\\DataPipeline\\\\eval.csv', TEST_DATA_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The location of the train data. C:\\Python_DS_Challenges\\DataPipeline\\train.csv\n",
      "The location of the test data. C:\\Python_DS_Challenges\\DataPipeline\\eval.csv\n"
     ]
    }
   ],
   "source": [
    "# See the location of the data\n",
    "\n",
    "print(\"The location of the train data.\" , train_file_path)\n",
    "print(\"The location of the test data.\" , test_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "\n",
    " - Specify the features we want to use\n",
    "   - in this data, we want to use the column called \" Survived\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COLUMN = 'survived'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(file_path, **kwargs):\n",
    "    dataset =tf.data.experimental.make_csv_dataset(\n",
    "        file_path, \n",
    "        batch_size=5,\n",
    "        label_name=LABEL_COLUMN,\n",
    "        na_value='?',\n",
    "        num_epochs=1,\n",
    "        ignore_errors=True,\n",
    "        **kwargs)\n",
    "    return dataset\n",
    "\n",
    "raw_train_data = get_dataset(train_file_path)\n",
    "raw_test_data = get_dataset(test_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (OrderedDict([(sex, (None,)), (age, (None,)), (n_siblings_spouses, (None,)), (parch, (None,)), (fare, (None,)), (class, (None,)), (deck, (None,)), (embark_town, (None,)), (alone, (None,))]), (None,)), types: (OrderedDict([(sex, tf.string), (age, tf.float32), (n_siblings_spouses, tf.int32), (parch, tf.int32), (fare, tf.float32), (class, tf.string), (deck, tf.string), (embark_town, tf.string), (alone, tf.string)]), tf.int32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(dataset):\n",
    "  for batch, label in dataset.take(1):\n",
    "    for key, value in batch.items():\n",
    "      print(\"{:20s}: {}\".format(key,value.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Each item in the dataset is a batch, represented as a tuple of (many examples, many labels). The data from the examples is organized in column-based tensors (rather than row-based tensors), each with as many elements as the batch size (5 in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex                 : [b'male' b'male' b'male' b'male' b'male']\n",
      "age                 : [28. 36. 26. 11. 42.]\n",
      "n_siblings_spouses  : [0 0 2 0 1]\n",
      "parch               : [0 0 0 0 0]\n",
      "fare                : [56.4958 10.5     8.6625 18.7875 27.    ]\n",
      "class               : [b'Third' b'Second' b'Third' b'Third' b'Second']\n",
      "deck                : [b'unknown' b'unknown' b'unknown' b'unknown' b'unknown']\n",
      "embark_town         : [b'Southampton' b'Southampton' b'Southampton' b'Cherbourg' b'Southampton']\n",
      "alone               : [b'y' b'y' b'n' b'y' b'n']\n"
     ]
    }
   ],
   "source": [
    "show_batch(raw_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the columns in the CSV are named. The dataset constructor will pick these names up automatically. If the file you are working with does not contain the column names in the first line, pass them in a list of strings to the column_names argument in the make_csv_dataset function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex                 : [b'male' b'female' b'female' b'female' b'male']\n",
      "age                 : [18. 58. 63. 51. 29.]\n",
      "n_siblings_spouses  : [0 0 0 1 1]\n",
      "parch               : [0 1 0 0 0]\n",
      "fare                : [ 13.     153.4625   9.5875  77.9583   7.0458]\n",
      "class               : [b'Second' b'First' b'Third' b'First' b'Third']\n",
      "deck                : [b'unknown' b'C' b'unknown' b'D' b'unknown']\n",
      "embark_town         : [b'Southampton' b'Southampton' b'Southampton' b'Southampton'\n",
      " b'Southampton']\n",
      "alone               : [b'y' b'n' b'y' b'n' b'n']\n"
     ]
    }
   ],
   "source": [
    "CSV_COLUMNS = ['survived', 'sex', 'age', 'n_siblings_spouses', 'parch', 'fare', 'class', 'deck', 'embark_town', 'alone']\n",
    "\n",
    "temp_dataset = get_dataset(train_file_path, column_names=CSV_COLUMNS)\n",
    "\n",
    "show_batch(temp_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This example is going to use all the available columns. If you need to omit some columns from the dataset, create a list of just the columns you plan to use, and pass it into the (optional) select_columns argument of the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                 : [30. 28. 16. 25. 28.]\n",
      "n_siblings_spouses  : [0 8 0 1 0]\n",
      "class               : [b'First' b'Third' b'Third' b'Third' b'Third']\n",
      "deck                : [b'E' b'unknown' b'unknown' b'unknown' b'unknown']\n",
      "alone               : [b'y' b'n' b'y' b'n' b'y']\n"
     ]
    }
   ],
   "source": [
    "SELECT_COLUMNS = ['survived', 'age', 'n_siblings_spouses', 'class', 'deck', 'alone']\n",
    "\n",
    "temp_dataset = get_dataset(train_file_path, select_columns=SELECT_COLUMNS)\n",
    "\n",
    "show_batch(temp_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A CSV file can contain a variety of data types. Typically you want to convert from those mixed types to a fixed length vector before feeding the data into your model.\n",
    "\n",
    "TensorFlow has a built-in system for describing common input conversions: tf.feature_column, see this tutorial for details.\n",
    "\n",
    "You can preprocess your data using any tool you like (like nltk or sklearn), and just pass the processed output to TensorFlow.\n",
    "\n",
    "The primary advantage of doing the preprocessing inside your model is that when you export the model it includes the preprocessing. This way you can pass the raw data directly to your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If your data is already in an appropriate numeric format, you can pack the data into a vector before passing it off to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                 : [28. 28.  4. 28. 49.]\n",
      "n_siblings_spouses  : [0. 1. 1. 0. 1.]\n",
      "parch               : [0. 0. 1. 0. 1.]\n",
      "fare                : [  8.05    15.5     23.       7.8958 110.8833]\n"
     ]
    }
   ],
   "source": [
    "SELECT_COLUMNS = ['survived', 'age', 'n_siblings_spouses', 'parch', 'fare']\n",
    "DEFAULTS = [0, 0.0, 0.0, 0.0, 0.0]\n",
    "temp_dataset = get_dataset(train_file_path, \n",
    "                           select_columns=SELECT_COLUMNS,\n",
    "                           column_defaults = DEFAULTS)\n",
    "\n",
    "show_batch(temp_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch, labels_batch = next(iter(temp_dataset)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Here's a simple function that will pack together all the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack(features, label):\n",
    "  return tf.stack(list(features.values()), axis=-1), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35.      0.      0.     10.5   ]\n",
      " [24.      0.      2.     14.5   ]\n",
      " [20.      0.      0.      9.225 ]\n",
      " [28.      0.      0.     26.55  ]\n",
      " [27.      0.      0.      7.8958]]\n",
      "\n",
      "[0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "packed_dataset = temp_dataset.map(pack)\n",
    "\n",
    "for features, labels in packed_dataset.take(1):\n",
    "  print(features.numpy())\n",
    "  print()\n",
    "  print(labels.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If you have mixed datatypes you may want to separate out these simple-numeric fields. The tf.feature_column api can handle them, but this incurs some overhead and should be avoided unless really necessary. Switch back to the mixed dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex                 : [b'female' b'female' b'female' b'male' b'male']\n",
      "age                 : [24. 14. 30. 28. 28.]\n",
      "n_siblings_spouses  : [2 1 0 0 0]\n",
      "parch               : [3 2 0 0 0]\n",
      "fare                : [ 18.75   120.      12.475    7.8958   7.7292]\n",
      "class               : [b'Second' b'First' b'Third' b'Third' b'Third']\n",
      "deck                : [b'unknown' b'B' b'unknown' b'unknown' b'unknown']\n",
      "embark_town         : [b'Southampton' b'Southampton' b'Southampton' b'Cherbourg' b'Queenstown']\n",
      "alone               : [b'n' b'n' b'y' b'y' b'y']\n"
     ]
    }
   ],
   "source": [
    "show_batch(raw_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch, labels_batch = next(iter(temp_dataset)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " So define a more general preprocessor that selects a list of numeric features and packs them into a single column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PackNumericFeatures(object):\n",
    "  def __init__(self, names):\n",
    "    self.names = names\n",
    "\n",
    "  def __call__(self, features, labels):\n",
    "    numeric_features = [features.pop(name) for name in self.names]\n",
    "    numeric_features = [tf.cast(feat, tf.float32) for feat in numeric_features]\n",
    "    numeric_features = tf.stack(numeric_features, axis=-1)\n",
    "    features['numeric'] = numeric_features\n",
    "\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_FEATURES = ['age','n_siblings_spouses','parch', 'fare']\n",
    "\n",
    "packed_train_data = raw_train_data.map(\n",
    "    PackNumericFeatures(NUMERIC_FEATURES))\n",
    "\n",
    "packed_test_data = raw_test_data.map(\n",
    "    PackNumericFeatures(NUMERIC_FEATURES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex                 : [b'female' b'female' b'male' b'female' b'female']\n",
      "class               : [b'First' b'First' b'Third' b'First' b'Second']\n",
      "deck                : [b'B' b'C' b'unknown' b'C' b'E']\n",
      "embark_town         : [b'Southampton' b'Southampton' b'Southampton' b'Southampton' b'Queenstown']\n",
      "alone               : [b'y' b'n' b'y' b'n' b'y']\n",
      "numeric             : [[ 29.       0.       0.     211.3375]\n",
      " [ 25.       1.       2.     151.55  ]\n",
      " [ 19.       0.       0.       8.1583]\n",
      " [ 31.       0.       2.     164.8667]\n",
      " [ 28.       0.       0.      12.35  ]]\n"
     ]
    }
   ],
   "source": [
    "show_batch(packed_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch, labels_batch = next(iter(packed_train_data)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Continuous data should always be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>n_siblings_spouses</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>627.000000</td>\n",
       "      <td>627.000000</td>\n",
       "      <td>627.000000</td>\n",
       "      <td>627.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>29.631308</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.379585</td>\n",
       "      <td>34.385399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.511818</td>\n",
       "      <td>1.151090</td>\n",
       "      <td>0.792999</td>\n",
       "      <td>54.597730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.895800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.045800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.387500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age  n_siblings_spouses       parch        fare\n",
       "count  627.000000          627.000000  627.000000  627.000000\n",
       "mean    29.631308            0.545455    0.379585   34.385399\n",
       "std     12.511818            1.151090    0.792999   54.597730\n",
       "min      0.750000            0.000000    0.000000    0.000000\n",
       "25%     23.000000            0.000000    0.000000    7.895800\n",
       "50%     28.000000            0.000000    0.000000   15.045800\n",
       "75%     35.000000            1.000000    0.000000   31.387500\n",
       "max     80.000000            8.000000    5.000000  512.329200"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "desc = pd.read_csv(train_file_path)[NUMERIC_FEATURES].describe()\n",
    "desc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = np.array(desc.T['mean'])\n",
    "STD = np.array(desc.T['std'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# def normalize_numeric_data(data, mean, std):\n",
    "  # Center the data\n",
    "  return (data-mean)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now create a numeric column. The tf.feature_columns.numeric_column API accepts a normalizer_fn argument, which will be run on each batch.\n",
    "\n",
    "Bind the MEAN and STD to the normalizer fn using functools.partial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NumericColumn(key='numeric', shape=(4,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function normalize_numeric_data at 0x00000150C4D96318>, mean=array([29.63130781,  0.54545455,  0.37958533, 34.38539856]), std=array([12.51181763,  1.1510896 ,  0.79299921, 54.5977305 ])))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what you just created.\n",
    "import functools\n",
    "normalizer = functools.partial(normalize_numeric_data, mean=MEAN, std=STD)\n",
    "\n",
    "numeric_column = tf.feature_column.numeric_column('numeric', normalizer_fn=normalizer, shape=[len(NUMERIC_FEATURES)])\n",
    "numeric_columns = [numeric_column]\n",
    "numeric_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " When you train the model, include this feature column to select and center this block of numeric data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=874, shape=(5, 4), dtype=float32, numpy=\n",
       "array([[21.    ,  0.    ,  0.    , 77.9583],\n",
       "       [11.    ,  4.    ,  2.    , 31.275 ],\n",
       "       [28.    ,  1.    ,  0.    ,  7.75  ],\n",
       "       [46.    ,  1.    ,  0.    , 61.175 ],\n",
       "       [28.    ,  0.    ,  0.    ,  8.05  ]], dtype=float32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch['numeric']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.6898524 , -0.47385937, -0.4786705 ,  0.7980716 ],\n",
       "       [-1.4890968 ,  3.0011094 ,  2.0434003 , -0.05696939],\n",
       "       [-0.13038135,  0.39488277, -0.4786705 , -0.4878481 ],\n",
       "       [ 1.3082585 ,  0.39488277, -0.4786705 ,  0.49067244],\n",
       "       [-0.13038135, -0.47385937, -0.4786705 , -0.4823534 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_layer = tf.keras.layers.DenseFeatures(numeric_columns)\n",
    "numeric_layer(example_batch).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean based normalization used here requires knowing the means of each column ahead of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Some of the columns in the CSV data are categorical columns. That is, the content should be one of a limited set of \n",
    "options.\n",
    "\n",
    "\n",
    "Use the tf.feature_column API to create a collection with a tf.feature_column.indicator_column for each categorical column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = {\n",
    "    'sex': ['male', 'female'],\n",
    "    'class' : ['First', 'Second', 'Third'],\n",
    "    'deck' : ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'],\n",
    "    'embark_town' : ['Cherbourg', 'Southhampton', 'Queenstown'],\n",
    "    'alone' : ['y', 'n']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = []\n",
    "for feature, vocab in CATEGORIES.items():\n",
    "  cat_col = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        key=feature, vocabulary_list=vocab)\n",
    "  categorical_columns.append(tf.feature_column.indicator_column(cat_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='sex', vocabulary_list=('male', 'female'), dtype=tf.string, default_value=-1, num_oov_buckets=0)),\n",
       " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='class', vocabulary_list=('First', 'Second', 'Third'), dtype=tf.string, default_value=-1, num_oov_buckets=0)),\n",
       " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='deck', vocabulary_list=('A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'), dtype=tf.string, default_value=-1, num_oov_buckets=0)),\n",
       " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='embark_town', vocabulary_list=('Cherbourg', 'Southhampton', 'Queenstown'), dtype=tf.string, default_value=-1, num_oov_buckets=0)),\n",
       " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='alone', vocabulary_list=('y', 'n'), dtype=tf.string, default_value=-1, num_oov_buckets=0))]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what you just created.\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\feature_column\\feature_column_v2.py:4276: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\feature_column\\feature_column_v2.py:4331: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "[1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "categorical_layer = tf.keras.layers.DenseFeatures(categorical_columns)\n",
    "print(categorical_layer(example_batch).numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be become part of a data processing input later when you build the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined preprocessing layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the two feature column collections and pass them to a tf.keras.layers.DenseFeatures to create an input layer that will extract and preprocess both input types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_layer = tf.keras.layers.DenseFeatures(categorical_columns+numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.          0.          1.          0.          0.          0.\n",
      "  0.          0.          1.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      " -0.6898524  -0.47385937 -0.4786705   0.7980716   0.          1.        ]\n"
     ]
    }
   ],
   "source": [
    "print(preprocessing_layer(example_batch).numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Build a tf.keras.Sequential, starting with the preprocessing_layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  preprocessing_layer,\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, evaluate, and predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now the model can be instantiated and trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = packed_train_data.shuffle(500)\n",
    "test_data = packed_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "126/126 [==============================] - 5s 41ms/step - loss: 0.5074 - accuracy: 0.7257\n",
      "Epoch 2/20\n",
      "126/126 [==============================] - 1s 5ms/step - loss: 0.4314 - accuracy: 0.8022\n",
      "Epoch 3/20\n",
      "126/126 [==============================] - 1s 5ms/step - loss: 0.4021 - accuracy: 0.8166\n",
      "Epoch 4/20\n",
      "126/126 [==============================] - 1s 5ms/step - loss: 0.3930 - accuracy: 0.8389\n",
      "Epoch 5/20\n",
      "126/126 [==============================] - 1s 5ms/step - loss: 0.3836 - accuracy: 0.8309\n",
      "Epoch 6/20\n",
      "126/126 [==============================] - 1s 5ms/step - loss: 0.3678 - accuracy: 0.8373\n",
      "Epoch 7/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.3582 - accuracy: 0.8453\n",
      "Epoch 8/20\n",
      "126/126 [==============================] - 1s 5ms/step - loss: 0.3557 - accuracy: 0.8437\n",
      "Epoch 9/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.3470 - accuracy: 0.8453\n",
      "Epoch 10/20\n",
      "126/126 [==============================] - 1s 5ms/step - loss: 0.3465 - accuracy: 0.8469\n",
      "Epoch 11/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.3361 - accuracy: 0.8533\n",
      "Epoch 12/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.3325 - accuracy: 0.8517\n",
      "Epoch 13/20\n",
      "126/126 [==============================] - 1s 5ms/step - loss: 0.3240 - accuracy: 0.8565\n",
      "Epoch 14/20\n",
      "126/126 [==============================] - 1s 5ms/step - loss: 0.3217 - accuracy: 0.8549\n",
      "Epoch 15/20\n",
      "126/126 [==============================] - 1s 5ms/step - loss: 0.3202 - accuracy: 0.8644\n",
      "Epoch 16/20\n",
      "126/126 [==============================] - 1s 5ms/step - loss: 0.3138 - accuracy: 0.8596\n",
      "Epoch 17/20\n",
      "126/126 [==============================] - 1s 5ms/step - loss: 0.3083 - accuracy: 0.8581\n",
      "Epoch 18/20\n",
      "126/126 [==============================] - 1s 5ms/step - loss: 0.3147 - accuracy: 0.8549\n",
      "Epoch 19/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.3045 - accuracy: 0.8692\n",
      "Epoch 20/20\n",
      "126/126 [==============================] - 1s 5ms/step - loss: 0.3008 - accuracy: 0.8676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x150c4bc6948>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Once the model is trained, you can check its accuracy on the test_data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 1s 15ms/step - loss: 0.4617 - accuracy: 0.8447\n",
      "\n",
      "\n",
      "Test Loss 0.4616721570491791, Test Accuracy 0.8446969985961914\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)\n",
    "\n",
    "print('\\n\\nTest Loss {}, Test Accuracy {}'.format(test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Use tf.keras.Model.predict to infer labels on a batch or a dataset of batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted survival: 6.39%  | Actual outcome:  SURVIVED\n",
      "Predicted survival: 99.54%  | Actual outcome:  DIED\n",
      "Predicted survival: 9.97%  | Actual outcome:  DIED\n",
      "Predicted survival: 24.69%  | Actual outcome:  SURVIVED\n",
      "Predicted survival: 92.52%  | Actual outcome:  SURVIVED\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_data)\n",
    "\n",
    "# Show some results\n",
    "for prediction, survived in zip(predictions[:10], list(test_data)[0][1][:10]):\n",
    "  prediction = tf.sigmoid(prediction).numpy()\n",
    "  print(\"Predicted survival: {:.2%}\".format(prediction[0]),\n",
    "        \" | Actual outcome: \",\n",
    "        (\"SURVIVED\" if bool(survived) else \"DIED\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load NumPy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data from NumPy arrays into a tf.data.Dataset.\n",
    "This example loads the MNIST dataset from a .npz file. However, the source of the NumPy arrays is not important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from .npz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "DATA_URL = 'https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz'\n",
    "\n",
    "path = tf.keras.utils.get_file('mnist.npz', DATA_URL)\n",
    "with np.load(path) as data:\n",
    "  train_examples = data['x_train']\n",
    "  train_labels = data['y_train']\n",
    "  test_examples = data['x_test']\n",
    "  test_labels = data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load NumPy arrays with tf.data.Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming you have an array of examples and a corresponding array of labels, pass the two arrays as a tuple into tf.data.Dataset.from_tensor_slices to create a tf.data.Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_examples, train_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_examples, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle and batch the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['sparse_categorical_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 8s 9ms/step - loss: 3.4820 - sparse_categorical_accuracy: 0.87612.67 - 2s 15ms/step - los - 3s 13ms/s - 3s 12ms/step - loss:  - 4s 11ms/step - loss: 6.6921 - sparse_categorica\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.5268 - sparse_categorical_accuracy: 0.9254\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 4s 5ms/step - loss: 0.3851 - sparse_categorical_accuracy: 0.9453\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.3262 - sparse_categorical_accuracy: 0.9539\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.2833 - sparse_categorical_accuracy: 0.9605\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 6s 7ms/step - loss: 0.2626 - sparse_categorical_accuracy: 0.9648\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.2327 - sparse_categorical_accuracy: 0.9687\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.2200 - sparse_categorical_accuracy: 0.9698\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.2110 - sparse_categorical_accuracy: 0.9733\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.2015 - sparse_categorical_accuracy: 0.9741\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x150c555bac8>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a pandas.DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use a small dataset provided by the Cleveland Clinic Foundation for Heart Disease. There are several hundred rows in the CSV. Each row describes a patient, and each column describes an attribute. We will use this information to predict whether a patient has heart disease, which in this dataset is a binary classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the csv file containing the heart dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/applied-dl/heart.csv\n",
      "16384/13273 [=====================================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "csv_file = tf.keras.utils.get_file('heart.csv', 'https://storage.googleapis.com/applied-dl/heart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>fixed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>normal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>120</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>reversible</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   1       145   233    1        2      150      0      2.3      3   \n",
       "1   67    1   4       160   286    0        2      108      1      1.5      2   \n",
       "2   67    1   4       120   229    0        2      129      1      2.6      2   \n",
       "3   37    1   3       130   250    0        0      187      0      3.5      3   \n",
       "4   41    0   2       130   204    0        2      172      0      1.4      1   \n",
       "\n",
       "   ca        thal  target  \n",
       "0   0       fixed       0  \n",
       "1   3      normal       1  \n",
       "2   2  reversible       0  \n",
       "3   0      normal       0  \n",
       "4   0      normal       0  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age           int64\n",
       "sex           int64\n",
       "cp            int64\n",
       "trestbps      int64\n",
       "chol          int64\n",
       "fbs           int64\n",
       "restecg       int64\n",
       "thalach       int64\n",
       "exang         int64\n",
       "oldpeak     float64\n",
       "slope         int64\n",
       "ca            int64\n",
       "thal         object\n",
       "target        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert thal column which is an object in the dataframe to a discrete numerical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['thal'] = pd.Categorical(df['thal'])\n",
    "df['thal'] = df.thal.cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>120</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   1       145   233    1        2      150      0      2.3      3   \n",
       "1   67    1   4       160   286    0        2      108      1      1.5      2   \n",
       "2   67    1   4       120   229    0        2      129      1      2.6      2   \n",
       "3   37    1   3       130   250    0        0      187      0      3.5      3   \n",
       "4   41    0   2       130   204    0        2      172      0      1.4      1   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     2       0  \n",
       "1   3     3       1  \n",
       "2   2     4       0  \n",
       "3   0     3       0  \n",
       "4   0     3       0  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data using tf.data.Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use tf.data.Dataset.from_tensor_slices to read the values from a pandas dataframe.\n",
    "\n",
    "One of the advantages of using tf.data.Dataset is it allows you to write simple, highly efficient data pipelines. Read the loading data guide to find out more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df.pop('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((df.values, target.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: [ 63.    1.    1.  145.  233.    1.    2.  150.    0.    2.3   3.    0.\n",
      "   2. ], Target: 0\n",
      "Features: [ 67.    1.    4.  160.  286.    0.    2.  108.    1.    1.5   2.    3.\n",
      "   3. ], Target: 1\n",
      "Features: [ 67.    1.    4.  120.  229.    0.    2.  129.    1.    2.6   2.    2.\n",
      "   4. ], Target: 0\n",
      "Features: [ 37.    1.    3.  130.  250.    0.    0.  187.    0.    3.5   3.    0.\n",
      "   3. ], Target: 0\n",
      "Features: [ 41.    0.    2.  130.  204.    0.    2.  172.    0.    1.4   1.    0.\n",
      "   3. ], Target: 0\n"
     ]
    }
   ],
   "source": [
    "for feat, targ in dataset.take(5):\n",
    "  print ('Features: {}, Target: {}'.format(feat, targ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Since a pd.Series implements the __array__ protocol it can be used transparently nearly anywhere you would use a np.array or a tf.Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=40426, shape=(303,), dtype=int32, numpy=\n",
       "array([2, 3, 4, 3, 3, 3, 3, 3, 4, 4, 2, 3, 2, 4, 4, 3, 4, 3, 3, 3, 3, 3,\n",
       "       3, 4, 4, 3, 3, 3, 3, 4, 3, 4, 3, 4, 3, 3, 4, 2, 4, 3, 4, 3, 4, 4,\n",
       "       2, 3, 3, 4, 3, 3, 4, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 4, 4, 3, 3, 4,\n",
       "       4, 2, 3, 3, 4, 3, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 3, 4, 4, 4,\n",
       "       3, 3, 4, 3, 4, 4, 3, 4, 3, 3, 3, 4, 3, 4, 4, 3, 3, 4, 4, 4, 4, 4,\n",
       "       3, 3, 3, 3, 4, 3, 4, 3, 4, 4, 3, 3, 2, 4, 4, 2, 3, 3, 4, 4, 3, 4,\n",
       "       3, 3, 4, 2, 4, 4, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4,\n",
       "       4, 3, 3, 3, 4, 3, 4, 3, 4, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 4, 3, 2,\n",
       "       4, 4, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 2, 2, 4, 3, 4, 2, 4, 3,\n",
       "       3, 4, 3, 3, 3, 3, 4, 3, 4, 3, 4, 2, 2, 4, 3, 4, 3, 2, 4, 3, 3, 2,\n",
       "       4, 4, 4, 4, 3, 0, 3, 3, 3, 3, 1, 4, 3, 3, 3, 4, 3, 4, 3, 3, 3, 4,\n",
       "       3, 3, 4, 4, 4, 4, 3, 3, 4, 3, 4, 3, 4, 4, 3, 4, 4, 3, 4, 4, 3, 3,\n",
       "       3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 3, 2, 4, 4, 4, 4])>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(df['thal'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Shuffle and batch the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.shuffle(len(df)).batch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compiled_model():\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer sequential_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1/15\n",
      "303/303 [==============================] - 3s 9ms/step - loss: 2.3332 - accuracy: 0.6733876 - \n",
      "Epoch 2/15\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 1.1261 - accuracy: 0.6931\n",
      "Epoch 3/15\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9030 - accuracy: 0.6832\n",
      "Epoch 4/15\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.7757 - accuracy: 0.6931\n",
      "Epoch 5/15\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6175 - accuracy: 0.7195\n",
      "Epoch 6/15\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.5570 - accuracy: 0.7096\n",
      "Epoch 7/15\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.5337 - accuracy: 0.7360\n",
      "Epoch 8/15\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.5315 - accuracy: 0.7492\n",
      "Epoch 9/15\n",
      "303/303 [==============================] - 1s 4ms/step - loss: 0.5254 - accuracy: 0.7360: 0s - loss: 0.4736 - accuracy:  - ETA: 0s - loss:\n",
      "Epoch 10/15\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.4691 - accuracy: 0.7459\n",
      "Epoch 11/15\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.4808 - accuracy: 0.7657\n",
      "Epoch 12/15\n",
      "303/303 [==============================] - 1s 4ms/step - loss: 0.4744 - accuracy: 0.7855\n",
      "Epoch 13/15\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.4844 - accuracy: 0.7624\n",
      "Epoch 14/15\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.4510 - accuracy: 0.7492\n",
      "Epoch 15/15\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.4589 - accuracy: 0.7921\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x150caffb508>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = get_compiled_model()\n",
    "model.fit(train_dataset, epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative to feature columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing a dictionary as an input to a model is as easy as creating a matching dictionary of tf.keras.layers.Input layers, applying any pre-processing and stacking them up using the functional api. You can use this as an alternative to feature columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {key: tf.keras.layers.Input(shape=(), name=key) for key in df.keys()}\n",
    "x = tf.stack(list(inputs.values()), axis=-1)\n",
    "\n",
    "x = tf.keras.layers.Dense(10, activation='relu')(x)\n",
    "output = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "model_func = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "model_func.compile(optimizer='adam',\n",
    "                   loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The easiest way to preserve the column structure of a pd.DataFrame when used with tf.data is to convert the pd.DataFrame to a dict, and slice that dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_slices = tf.data.Dataset.from_tensor_slices((df.to_dict('list'), target.values)).batch(16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'age': <tf.Tensor: id=55148, shape=(16,), dtype=int32, numpy=array([63, 67, 67, 37, 41, 56, 62, 57, 63, 53, 57, 56, 56, 44, 52, 57])>, 'sex': <tf.Tensor: id=55156, shape=(16,), dtype=int32, numpy=array([1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1])>, 'cp': <tf.Tensor: id=55151, shape=(16,), dtype=int32, numpy=array([1, 4, 4, 3, 2, 2, 4, 4, 4, 4, 4, 2, 3, 2, 3, 3])>, 'trestbps': <tf.Tensor: id=55160, shape=(16,), dtype=int32, numpy=\n",
      "array([145, 160, 120, 130, 130, 120, 140, 120, 130, 140, 140, 140, 130,\n",
      "       120, 172, 150])>, 'chol': <tf.Tensor: id=55150, shape=(16,), dtype=int32, numpy=\n",
      "array([233, 286, 229, 250, 204, 236, 268, 354, 254, 203, 192, 294, 256,\n",
      "       263, 199, 168])>, 'fbs': <tf.Tensor: id=55153, shape=(16,), dtype=int32, numpy=array([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0])>, 'restecg': <tf.Tensor: id=55155, shape=(16,), dtype=int32, numpy=array([2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 0, 2, 2, 0, 0, 0])>, 'thalach': <tf.Tensor: id=55159, shape=(16,), dtype=int32, numpy=\n",
      "array([150, 108, 129, 187, 172, 178, 160, 163, 147, 155, 148, 153, 142,\n",
      "       173, 162, 174])>, 'exang': <tf.Tensor: id=55152, shape=(16,), dtype=int32, numpy=array([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0])>, 'oldpeak': <tf.Tensor: id=55154, shape=(16,), dtype=float32, numpy=\n",
      "array([2.3, 1.5, 2.6, 3.5, 1.4, 0.8, 3.6, 0.6, 1.4, 3.1, 0.4, 1.3, 0.6,\n",
      "       0. , 0.5, 1.6], dtype=float32)>, 'slope': <tf.Tensor: id=55157, shape=(16,), dtype=int32, numpy=array([3, 2, 2, 3, 1, 1, 3, 1, 2, 3, 2, 2, 2, 1, 1, 1])>, 'ca': <tf.Tensor: id=55149, shape=(16,), dtype=int32, numpy=array([0, 3, 2, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0])>, 'thal': <tf.Tensor: id=55158, shape=(16,), dtype=int32, numpy=array([2, 3, 4, 3, 3, 3, 3, 3, 4, 4, 2, 3, 2, 4, 4, 3])>}, <tf.Tensor: id=55161, shape=(16,), dtype=int64, numpy=array([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0], dtype=int64)>)\n"
     ]
    }
   ],
   "source": [
    "for dict_slice in dict_slices.take(1):\n",
    "  print (dict_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "19/19 [==============================] - 1s 51ms/step - loss: 5.2082 - accuracy: 0.4290\n",
      "Epoch 2/15\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 3.5295 - accuracy: 0.5875\n",
      "Epoch 3/15\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 2.5101 - accuracy: 0.5446\n",
      "Epoch 4/15\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 1.7102 - accuracy: 0.6073\n",
      "Epoch 5/15\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 1.2429 - accuracy: 0.6502\n",
      "Epoch 6/15\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.9969 - accuracy: 0.6997\n",
      "Epoch 7/15\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.8739 - accuracy: 0.7162\n",
      "Epoch 8/15\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.8051 - accuracy: 0.7228\n",
      "Epoch 9/15\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.7579 - accuracy: 0.7393\n",
      "Epoch 10/15\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.7228 - accuracy: 0.7459\n",
      "Epoch 11/15\n",
      "19/19 [==============================] - 0s 11ms/step - loss: 0.6951 - accuracy: 0.7492\n",
      "Epoch 12/15\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.6723 - accuracy: 0.7591\n",
      "Epoch 13/15\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.6515 - accuracy: 0.7690\n",
      "Epoch 14/15\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.6345 - accuracy: 0.7822\n",
      "Epoch 15/15\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.6210 - accuracy: 0.7888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x150cb68c2c8>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_func.fit(dict_slices, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
