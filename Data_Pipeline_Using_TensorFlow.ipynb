{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing Data Pipeline with TensorFlow 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow is one of the most loved and widely adopted Machine Learning platforms.Many people use it and there were some great additions to TensorFlow 2.0, specifically tf.data for optimizating data pipelines.\n",
    "\n",
    "\n",
    "In this module I am going to go over \n",
    "\n",
    "###  -TensorFlow 2.0 New Features\n",
    "### - TensorFlow's data module\n",
    "### - Migration from TF 1.0 to TF 2.0\n",
    "### - Dataset and Keras Overview\n",
    "\n",
    "### End Result:\n",
    " - Setting Stage for building efficient data pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF 2.0 Introduction  \n",
    " \n",
    "\n",
    "### - Easy Model Building\n",
    "This encompasses few eliments\n",
    "  - Combination of Keras API\n",
    "  - Robust Deployment Into Production\n",
    "  - Powerful Untilities\n",
    "  - Imporved Data Pipelines\n",
    " \n",
    " # Tf.data \n",
    " New in TensorFlow 2.0\n",
    " \n",
    " - Handle large amount of data\n",
    " - Easier Demplyment\n",
    " - Simplified API\n",
    " - Performance Optimization\n",
    "   - Read from different files formats\n",
    "   - Complex transformation\n",
    "   - Easy Reading and parallelization\n",
    "   \n",
    "   \n",
    " NOTE: # make tensorFlow 2.0 backward compatibility by using tf.disable_v2_behaviour().  This allow to use TensorFlow 1.0 (tf.1) However by doing this we no longer able to access TensorFlow 2.0 peformance improvements specially tf.data\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipleline and Model Training\n",
    "\n",
    "\n",
    "    ### This is a general overview and process of building data pipeline efficently using TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with importing libraries \n",
    "# import tensorflow with the standard alias tf.  \n",
    "# import numpy just for some utility functions\n",
    "# import keras, this is one of the standard methods used commonly, we can use tf.keras as well\n",
    "\n",
    "import tensorflow as tf # For tensor operations\n",
    "import numpy as np#arrays  \n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fasion_mnist dataset\n",
    "fashin_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "# take the fashion_mnist data object and load the data method on that object\n",
    "(train_images, train_labels), (test_images, test_labels) = fashin_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the shape of the train images data, this is going to be three values - we have 60000 observations\n",
    "# these are two dimensional data, we have axis for each of those values ( we have no obervations, and two dimensions\n",
    "# on top of those observations)\n",
    "# we have 28 x 28 pixels in each image\n",
    "\n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data, because each of these values inside of the 28 pixelhas a value of between 0 and 255\n",
    "# This is common rule in dealing with image data\n",
    "train_images = train_images / 255\n",
    "test_images = test_images / 255 \n",
    "\n",
    "# this is acutally scaling the data between and the value is going to rang between 0 - 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Model\n",
    "\n",
    "- Here we are going to use Squential element of Keras. This API inside Keras help add in squentially each step that we would like to work on\n",
    "\n",
    "### First Step\n",
    "- First step is going to be Flatten(). Flatten is a requirement in the image data(its not a requirement in non-image data).\n",
    "      and we pass in the input_shape. The input shape here is 28 x 28 ( we need to tell data what shape is going in)\n",
    "### Second Step\n",
    "- The second step is going to be creating a Dense layer. This is one of the intermediate step between the input nodes and the  output nodes. We can use 128 which is common in small image data and the activation function \"RELU\".\n",
    " \n",
    "### Third Step\n",
    " - The last layer is going to be another dense layer, we are going to pass in the value as 10.  In the given data there are 10 different values or classes of objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compling The Model\n",
    "\n",
    "#### We created the moel objects with our layers and the next step is how we want to train the model.\n",
    " ######  - \"ADAM OPTIMIZER\"  \n",
    "        - We are going to use the \"Adam\" Optimizer, \n",
    "        \n",
    " ######  - \"LOSS - SparseCategoricalCrossEntropy from logit\"\n",
    "        - It means, we want to create our outputs of a logistic regression output\n",
    "        \n",
    "  ######  -  \"METICS - accuracy\"  \n",
    " \n",
    "- We want to optimize on the amount of loss that we have through cross entropy and then metrics that we are measuring is going to be the accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer ='adam',\n",
    "             loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting The Model\n",
    "\n",
    "I am training the model on CPU, so its going to be little bit slower. \n",
    "There are 10 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.5015 - accuracy: 0.8247\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.3763 - accuracy: 0.8649\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.3419 - accuracy: 0.8752\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.3172 - accuracy: 0.8837\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.2981 - accuracy: 0.8899\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.2841 - accuracy: 0.8950\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.2713 - accuracy: 0.8999\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.2588 - accuracy: 0.9031\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.2496 - accuracy: 0.9071\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.2415 - accuracy: 0.9082\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ac9cac3908>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With this simply build 10 epocs - got the accuracy of 90% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "##### - We are going to simply use the predict method to predict the model and pass in the test images and that is going to give us the actual predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -9.326098  , -17.924871  , -13.18767   , -26.877266  ,\n",
       "       -11.62606   ,  -2.0717914 , -11.633827  ,  -0.38116384,\n",
       "       -11.328041  ,   5.1441145 ], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(test_images)\n",
    "predictions[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - These are logit values and we need to convert them into probabilities by taking the maximum values.This will give us what is this model going to predit in most likely case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test The Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
